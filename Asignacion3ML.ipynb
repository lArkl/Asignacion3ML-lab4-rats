{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignacion 3\n",
    "\n",
    "Todos los asignaciones serán presentadas utilizando los cuadernos de [Jupyter Notebook](http://jupyter.org/), además de respectivas pruebas como ejemplo, así como el uso de mediciones de velocidad de ejecución utilizando el comando `timeit` y algunos gráficos si fuese necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preguntas 1\n",
    "\n",
    "\n",
    "1 . Sea $C \\subseteq \\mathbf{R}^n$ un conjunto convexo, con $x_1, x_2, \\dots x_k \\in C$ y sean $\\theta_1, \\dots \\theta_k \\in \\mathbb{R}, \\theta_1 + \\dots + \\theta_k = 1$. Muestra que $\\theta_1x_1 + \\dots  \\theta_kx_k \\in C$.\n",
    "\n",
    "2 .Demuestra que un conjunto es convexo si y sólo si su intersección con cualquier línea es convexa. Demuestra que un conjunto es afín si y sólo si su intersección con cualquier línea es afín.\n",
    "\n",
    "3 . Convexidad del punto medio. Un conjunto C es convexo del punto medio, si para cada dos puntos $a$, $b$ están en $C$, el punto medio o medio $(a + b)/2$ está en C. \n",
    "\n",
    "Obviamente un conjunto convexo es  convexo medio. Se puede probar que bajo condiciones suaves la convexidad del punto medio implica convexidad. Como un caso simple, prueba  que si C es cerrado y convexo medio, entonces C es convexo.\n",
    "\n",
    "4 .Demuestra que la cápsula convexo de un conjunto S es la intersección de todos los conjuntos convexos que contienen S.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tus respuestas\n",
    "\n",
    "#### Solución 1.1\n",
    "Como $C$ es un conjunto convexo, se debe cumplir que para un $\\lambda \\in [0,1]$:\n",
    "$$\n",
    "z_2 = \\lambda x_1 + (1-\\lambda) x_2\n",
    "$$\n",
    "$$\n",
    "z_2 \\in C\n",
    "$$\n",
    "\n",
    "Si $x_3\\in C$, se debe cumplir para un $\\beta \\in [0,1]$:\n",
    "\n",
    "$$\n",
    "z_3 = \\beta x_3+ (1-\\beta)z_2\\\\\n",
    "z_3 = \\beta x_3 + (\\lambda - \\lambda\\beta )x_1 + (1-\\beta - \\lambda+\\lambda\\beta) x_2\n",
    "$$\n",
    "Por condición, $z_3\\in C$, podemos hacer el cambio:\n",
    "$$\n",
    "\\beta = w_1\\\\\n",
    "\\lambda - \\lambda\\beta = w_2\\\\\n",
    "1-\\beta - \\lambda+\\lambda\\beta = w_3\n",
    "$$\n",
    "Notamos que $w_1+w_2+w_3 = 1$\n",
    "\n",
    "Hipotesis: $z_{k-1}\\in C$, donde $z_{k-1}$ es $w_1 x_1 + \\ldots w_{k-1} x_{k-1} \\in C$; $w_1+\\ldots + w_{k-1}=1$\n",
    "\n",
    "Como $x_k \\in C$, podemos expresar un $z_k$ como sigue:\n",
    "$$\n",
    "z_k = \\theta_{k} x_k+ (1-\\theta_k)z_{k-1}\\\\\n",
    "z_k = \\theta_k x_k + (1- \\theta_k  )w_1x_1 + (1 - \\theta_k )w_2 x_2+\\ldots+(1 - \\theta_k )w_{k-1}x_{k-1}\n",
    "$$\n",
    "\n",
    "Podemos reemplazar $\\theta_k - \\theta_k w_i$ por $\\theta_i$, resultando en lo siguiente:\n",
    "$$\n",
    "z_k = \\theta_k x_k + \\theta_1 x_1 + \\ldots + \\theta_{k-1}x_{k-1}\n",
    "$$\n",
    "\n",
    "Verificamos que:\n",
    "$$\n",
    "\\theta_k+\\theta_1+\\ldots+\\theta_{k+1} = \\theta_k +(w_1+w_2+\\ldots+w_{k-1})(1-\\theta_k) = \\theta_k + 1 (1-\\theta_k) =1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Solución 1.2\n",
    "\n",
    "##### a)\n",
    "Siendo $C$ un conjunto convexo y $L$ una línea cualquiera:\n",
    "$$\n",
    "C \\cap L= M\n",
    "$$\n",
    "Para $M=\\emptyset$, trivial.\n",
    "\n",
    "($\\Rightarrow$)\n",
    "\n",
    "Para $M\\neq\\emptyset$: si $x,y \\in M \\rightarrow x,y \\in C$ y $x,y \\in L$. Por ello $z = \\lambda x +(1-\\lambda)y\\in C$.\n",
    "\n",
    "Para $0\\leq\\lambda\\leq1$. Pero $z=\\lambda (x-y) + y$, es el punto de una recta: $z \\in L \\rightarrow z\\in M$. Como $z$ es un punto arbitrario, $M$ es convexo.\n",
    "\n",
    "($\\Leftarrow$)\n",
    "\n",
    "Sea $x,y \\in M$, como $M$ es convexo, se debe cumplir:\n",
    "$$\n",
    "z = \\lambda(x-y) + y \\in M\n",
    "$$\n",
    "Pero, como $L$ es una recta cualquiera y la intersección $M$ siempre es convexa, entonces $C$ es convexo.\n",
    "##### b)\n",
    "Un conjunto $A$ es afin si $\\theta x+(1-\\theta)y \\in A$ para $\\theta \\in R$\n",
    "\n",
    "Siendo $L$ una línea cualquiera, la intersección $A \\cap L = I$ \n",
    "\n",
    "$I = \\emptyset$, trivial.\n",
    "\n",
    "($\\Rightarrow$)\n",
    "\n",
    "Siendo $I \\neq \\emptyset$, para $x,y\\in I$ se cumple $x,y \\in L$ y $x,y \\in A$. Por ello $z = \\theta x+(1-\\theta)y \\in A$.\n",
    "Pero $z$ se puede expresar como $\\theta (x-y) + y$, el punto de una recta, por lo que $z \\in L \\rightarrow z \\in I$. Por ello la intersección $I$ es afín.\n",
    "\n",
    "($\\Leftarrow$)\n",
    "\n",
    "Como $I$ es afín, $x,y \\in I$ entonces existe $z = \\theta x+(1-\\theta)y \\in I$. Como $z \\in L$ y $L$ es una recta cualquiera y la intersección $I$ siempre es afín, entonces $A$ es afín."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Solución 1.3\n",
    "Como $C$ es convexo medio, siendo $a<b$ ambos puntos arbitrarios de $C$, siendo $t\\in [0,1]$ podemos decir que:\n",
    "$$\n",
    "t_0 = \\frac{a+b}{2} \\in C\\\\\n",
    "t_0 \\in t\n",
    "$$\n",
    "Nuevamente podemos decir por convexidad del punto medio que: \n",
    "$$\n",
    "t_1 = (a+t_0)/2 \\in C\\\\\n",
    "t'_1 = (t_0+b)/2 \\in C\n",
    "$$\n",
    "Observamos que también $t_1,t'_1\\in t$.\n",
    "Analogamente podemos construir intervalos cada vez más pequeños tomando puntos medios. Por ello, y al ser $C$ un conjunto cerrado (incluye los límites de cada intervalo), cualquier $t_k \\in t$ se encuentra en $C$. Luego, podemos decir:\n",
    "$$\n",
    "x = a t_k + (1-t_k)b\n",
    "$$\n",
    "Como $1-t_k \\in [0,1]\\rightarrow 1-t_k \\in t$, entonces $x\\in C$.\n",
    "Como $a,b$ son arbitrarios, $C$ es un conjunto convexo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Solución 1.4\n",
    "\n",
    "Sea $x_1,x_2 \\ldots x_n \\in S$, la cápsula $C(S) = \\{x/ x=\\sum_i^n \\lambda_i x_i; \\sum_i^n \\lambda_i = 1; \\lambda_i \\geq 0\\}$ \n",
    "\n",
    "($\\Rightarrow$)\n",
    "\n",
    "Podemos expandir $C(S)$ para generar conjuntos convexos $C_i$, lo que implica que  $x = \\sum_i^n \\lambda_i x_i \\in C_i \\rightarrow x \\in C_1 \\cap C_2 \\cap \\ldots \\cap C_k = I$; Así mismo, $S \\subset I$. Como $x$ es arbitrario y $x \\in C(S)\\rightarrow C(S)=I$\n",
    "\n",
    "($\\Leftarrow$)\n",
    "\n",
    "Siendo $C_1, C_2 \\ldots C_k$ conjuntos convexos que contienen a $S$, $x = \\sum_i^n \\lambda_i x_i \\in C_i$, para $0\\leq\\lambda\\leq 1$ e $0<i\\leq k$.\n",
    "\n",
    "Como $x \\in C_i \\rightarrow z \\in C_1 \\cap C_2 \\cap \\ldots \\cap C_k = I$, $I$ es convexo y contiene $S$. Notamos que el conjunto conformado por todos los elementos de $x$ es $C(S)$, por ello: $I = C(S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preguntas 2\n",
    "\n",
    "1 .¿ Cuál es la distancia entre dos hiperplanos paralelos $\\{ x \\in \\mathbf{R}^n\\ |\\   a^Tx =b_1 \\}$ y $\\{ x \\in \\mathbf{R}^n \\ |\\  a^Tx =b_2 \\}$?.\n",
    "\n",
    "2 . Sean $a$ y $b$ dos puntos distintos en $\\mathbf{R}^n$, muestra que el conjunto de puntos que satisface. Demuestra que el conjunto de todos los puntos que están más cerca (en la norma euclidiana) $a$ que a $b$, esto es, $\\{x\\ | \\Vert x -a \\Vert_2 \\leq \\Vert x -b \\Vert_2$, es un semiespacio. \n",
    "\n",
    "Describe explicítamente como una desigualdad de la forma $c^Tx \\leq d$. Realiza un gráfico.\n",
    "\n",
    "3 . Sea $x_0, \\dots, x_K \\in \\mathbf{R}^n$. Considera el conjunto de todos los puntos que están más cerca (en la norma euclidiana) $x_0$ que a $x_i$, esto es\n",
    "\n",
    "$$\n",
    "    V = \\{x \\in \\mathbf{R}^n\\ |\\ \\Vert x -x_0 \\Vert_2 \\leq \\Vert x -x_i\\Vert_2,\\ i =1, \\dots, K.\n",
    "$$\n",
    "\n",
    "V es llamada región de Voronoi alrededor de $x_0$ con respecto a $x_1,\\dots, x_K$.\n",
    "\n",
    "Muestra que V es un poliedro. Expresa V en la forma $V = \\{x\\ |\\ Ax \\preceq b \\}$.\n",
    "\n",
    "Por el contrario, dado un poliedro P con interior no vacío, muestra cómo encontrar $x_0, \\dots x_K$ para que el poliedro sea la región de Voronoi de $x_0$ con respecto a $x_1,\\dots, x_K$.\n",
    "\n",
    "4 . $C \\subseteq \\mathbf{R}^n$ el conjunto solución de una desigualdad cuadrática,\n",
    "\n",
    "$$\n",
    "C = \\{ x \\in \\mathbf{R}^n \\ |\\ x^TAx + b^Tx + c \\leq 0 \\},\n",
    "$$\n",
    "\n",
    "donde $A \\in \\mathbf{S}^n, b \\in \\mathbf{R}^n$ y $c \\in \\mathbf{R}$.\n",
    "\n",
    "\n",
    "* Muestra que C es convexo si $A \\succeq 0$.\n",
    "\n",
    "* Muestra que la intersección de C y el hiperplano definido por $g^Tx + h = 0$ (donde $g \\neq 0)$ es convexa si $A + \\lambda gg^T$ si $A +gg^T \\succeq 0$ para algún $\\lambda \\in \\mathbf{R}$.\n",
    "    \n",
    "5 . Sea X una variable aleatoria de valor real con $\\mathbf{P}(x = a_i) = p_i, i = 1,\\dots,n$, donde $a_1 < a_2 < \\cdots < a_n$. \n",
    "\n",
    "De esta manera $p\\in \\mathbf{R}^n$ se encuentra en la probabilidad estándar simplex $PS = \\{p\\ |\\ \\mathbf{1}^Tp = 1, p\\succeq  0\\}$. \n",
    "\n",
    "¿Cuáles de las siguientes condiciones son convexas en p?.\n",
    "\n",
    "* $\\alpha  \\leq \\mathbf{E}f(x) \\leq \\beta$, donde $\\mathbf{E}f(x)$ es el valor esperado, esto es, $\\mathbf{E}f(x) = \\sum_{i =1}^np_if(a_i)$. (La función $f: \\mathbf{R} \\rightarrow \\mathbf{R}$ es dada).\n",
    "\n",
    "\n",
    "* $\\mathbf{P}(X > \\alpha) \\leq \\beta$.\n",
    "\n",
    "\n",
    "* $\\mathbf{E}\\vert X^3 \\vert \\leq \\alpha \\mathbf{E}\\vert X \\vert$.\n",
    "\n",
    "\n",
    "* $\\mathbf{E}X^2 \\leq \\alpha $.\n",
    "\n",
    "\n",
    "* $\\mathbf{E}X^2 \\geq \\alpha $.\n",
    "\n",
    "\n",
    "* $\\mathbf{Var}(X) \\leq \\alpha$, donde $\\mathbf{Var}(X) = \\mathbf{E}(X - \\mathbf{E}X)^2$ es la varianza de $X$.\n",
    "\n",
    "\n",
    "* $\\mathbf{Var}(X) \\geq \\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tus respuestas\n",
    "\n",
    "#### Solución 2.1\n",
    "Sea $x_1$ un punto cualquiera del primer hiperplano, $L$ una recta que pasa a través de $x_1$ en dirección del vector normal $\\vec{a}$.\n",
    "La ecuación de $L$ está dada por $x_1+at$, para todo $t\\in{R}^n$.\n",
    "Para encontrar la intersección de $L$ con el segundo hiperplano hacemos:\n",
    "$$\n",
    "a^T(x_1+at)=b_2\\Leftrightarrow \\frac{t=(b_2-a^Tx_1)}{a^Ta}=\\frac{b_2-b_1}{a^Ta}\n",
    "$$\n",
    "\n",
    "Por lo tanto, el punto de intersección es:\n",
    "$$\n",
    "x_2=x_1+ \\frac{a(b_2-b_1)}{a^Ta}\n",
    "$$\n",
    "La distancia entre esos puntos es la distancia entre los dos hiperplanos:\n",
    "$$\n",
    "\\Vert{x_1-x_2}\\Vert=\\frac{|b_2-b_1|}{a^Ta}\\Vert{a}\\Vert=\\frac{|b_2-b_1|}{\\Vert{a}\\Vert}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### Solución 2.2\n",
    "Al hablar de normas hablamos de cantidades positivas, por lo que podemos decir:\n",
    "\n",
    "$$\n",
    "\\Vert{x-a}\\Vert \\leq \\Vert{x-b}\\Vert \\Leftrightarrow \\Vert{x-a}\\Vert^2 \\leq \\Vert{x-b}\\Vert^2\\\\\n",
    "\\Leftrightarrow(x-a)^T(x-a)\\leq (x-b)^T(x-b)\\\\\n",
    "\\Leftrightarrow x^Tx-2a^Tx+a^Ta \\leq x^Tx-2b^tx+b^tb\\\\\n",
    "\\Leftrightarrow 2(b-a)^Tx \\leq b^Tb-a^Ta\n",
    "$$\n",
    "\n",
    "Por lo tanto, el conjunto es un semiespacio.Tomamos $c=2(b-a)$ y $d=b^Tb-a^ta$\n",
    "\n",
    "#### Solución 2.3\n",
    "Como $x$ es más cercano a $x_0$ que a $x_i$ si y solo si:\n",
    "$$\n",
    "\\Vert{x-x_0}\\Vert_2 \\leq \\Vert{x-x_i}\\Vert \\Leftrightarrow (x-x_0)^T(x-x_0)\\leq(x-x_i)^T(x-x_i)\\\\\n",
    "            \\Leftrightarrow x^Tx-2x_0^Tx+x_0^Tx_0 \\leq x^Tx-2x_i^Tx+x_i^Tx_i\\\\\n",
    "            \\Leftrightarrow 2(x_i-x_0)^Tx \\leq x_i^Tx_i-x_0^Tx_0\n",
    "$$\n",
    "que define un semiespacio.\n",
    "Podemos expresar $V$ como $$V=\\lbrace x/Ax\\leq b \\rbrace$$ haciendo:\n",
    "$$\n",
    " A = 2\\begin{bmatrix}\n",
    "     x_1-x_0\\\\\n",
    "     x_2-x_0\\\\\n",
    "     \\vdots \\\\\n",
    "     x_k-x_0\n",
    "     \\end{bmatrix}\n",
    " $$\n",
    " ,\n",
    " \n",
    " $$\n",
    " b=\\begin{bmatrix}\n",
    "     x_1^Tx_1-x_0^Tx_0\\\\\n",
    "     x_2^Tx_2-x_0^Tx_0\\\\\n",
    "     \\vdots \\\\\n",
    "     x_k^Tx_k-x_0^Tx_0\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Por el contrario, suponiendo $V=\\lbrace x \\mid Ax \\leq b$ con $A \\in \\mathbf{R}^{k\\times n}$\n",
    "\n",
    "#### Solucion 2.4\n",
    "Sabemos que un conjunto $C$ es convexo, si y solo si, su intersección con una recta arbitraria $\\lbrace \\hat x+tv\\mid t \\in \\mathbf{R}\\rbrace$ es convexa.\n",
    "\n",
    "##### Primer ítem:\n",
    "Tenemos: $(\\hat x+tv)^TA(\\hat x+tv)+b^T(\\hat x+tv)+c=\\alpha t^2+ \\beta t+\\gamma $, donde:\n",
    "$$\n",
    "\\alpha=v^TAv,\\\\\n",
    "\\beta=b^Tv+2\\hat x^TAv,\\\\\n",
    "\\gamma=c+b^T\\hat x+\\hat x^TA\\hat x,\n",
    "$$\n",
    "\n",
    "Ahora, la intersección de $C$ con la recta definida por $\\hat x$ y $v$ es el conjunto $\\lbrace \\hat x+tv \\mid \\alpha t^2+\\beta t+\\gamma \\leq 0\\rbrace$, que es convexo si $\\alpha\\geq 0$. Esto se cumple para cualquier $v$, si $v^TAv\\geq0$ para todo $v$, es decir, $A\\geq0$.\n",
    "\n",
    "\n",
    "##### Segundo ítem:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Solución 2.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problemas 3\n",
    "\n",
    "1 . Supongamos que $C$ y $D$ son conjuntos disconexos de $\\mathbf{R}^n$. Considera el conjunto $(a,b) \\in \\mathbf{R}^{n+1}$, para el cual $a^T x \\leq b$ para todo $x \\in C$ y $a^Tx \\geq b$ para todo $x \\in D$. Muestra que este es un cono convexo.\n",
    "\n",
    "2 .Supongamos que C y D son dos conjuntos convexos que no se intersecan, es decir, $C \\cap D = \\emptyset$.\n",
    "\n",
    "Entonces existe un $a \\neq 0$ y $b$ tal que  $a^Tx \\leq  b$ para todo $x\\in C$ y un $A^Tx \\geq b$ para todo $x \\in D$. En otras palabras, la función afín $a^Tx - b$ no es positiva en C y no negativa en D.\n",
    "\n",
    "El hiperplano $\\{x\\ |\\ a^T x = b\\}$ se llama un hiperplano de separación para los conjuntos C y D o se dice que separa los conjuntos C y D\n",
    "\n",
    "3 .Escribe  un ejemplo de dos conjuntos convexos cerrados que son disjuntos pero no pueden ser estrictamente separados.\n",
    "\n",
    "4 .La función soporte de un conjunto $C \\subseteq \\mathbf{R}^n$ es definida como\n",
    "\n",
    "$$\n",
    "S_C(y) = \\sup\\{y^Tx\\ |\\ x \\in C\\}\n",
    "$$\n",
    "\n",
    "\n",
    "Supongamos que C y D son conjuntos convexos cerrados en $\\mathbf{R}^n$. Demuestra que $C = D$ si y sólo si sus funciones de soporte son iguales.\n",
    "\n",
    "5 .Supongamos que el conjunto C es cerrado, que tiene  interior no vacío y tiene un hiperplano de soporte en cada punto de su frontera. Demuestre que C es convexo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tus respuestas\n",
    "\n",
    "#### Solución 3.1\n",
    "Un cono convexo tiene por propiedad $\\alpha x+ \\beta y \\in A$ para $\\alpha , \\beta \\in R$ y $x,y \\in A$\n",
    "\n",
    "Siendo $(a,b)$ y $(r,s)\\in A$, $x \\in C$ y $y \\in D$, entonces:\n",
    "$$\n",
    "a^Tx\\leq b \\land a^Ty\\geq b \\\\\n",
    "r^Tx\\leq s \\land r^Ty\\geq s\n",
    "$$\n",
    "para $\\alpha , \\beta$ escalares $\\in R$, podemos observar que:\n",
    "$$\n",
    "\\alpha a^Tx\\leq \\alpha b \\land \\alpha a^Ty\\geq \\alpha b \\rightarrow \\alpha (a,b) \\in A\\\\\n",
    "\\beta r^Tx\\leq \\beta s \\land \\beta r^Ty\\geq \\beta s \\rightarrow \\beta (r,s) \\in A\n",
    "$$\n",
    "\n",
    "Ahora verificamos que:\n",
    "$$\n",
    "\\alpha a^Tx + \\beta r^Tx\\leq \\alpha b + \\beta s \\rightarrow \\alpha (a,b)+\\beta (r,s) \\in A\n",
    "$$\n",
    "\n",
    "Por lo tanto, A es un cono convexo.\n",
    "\n",
    "#### Solución 3.2\n",
    "#### Solución 3.3\n",
    "Podemos tomar los conjuntos $C = \\{x \\in R^2 / x \\leq 0\\}$ y $D = \\{x>0\\land y>0 \\in R^2 / xy \\geq 1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problemas 4\n",
    "\n",
    "1 .Muestra que una función continua $f:\\mathbf{R}^n \\rightarrow \\mathbf{R}^n$ es convexa si y sólo si para cada segmento lineal, el valor promedio sobre el segmento es menor o igual al promedio de los valores en los extremos de los segmentos. Para cada $x, y \\in \\mathbf{R}^n$,\n",
    "\n",
    "$$\n",
    "\\int_{0}^{1}f(x + \\lambda(y -x)d \\lambda \\leq \\frac{f(x) + f(y)}{2}.\n",
    "$$\n",
    "\n",
    "2 .Prueba que una función $f$ dos veces diferenciable es convexa si y sólo si su dominio es convexo y $\\nabla^2f(x) \\succeq 0$ para todo $x$ en el dominio de $f$.\n",
    "\n",
    "3 .Una función $\\psi: \\mathbf{R}^n \\rightarrow \\mathbf{R}^n$ es llamada monótona si para todo $x, y $ en el dominio $\\psi$,\n",
    "\n",
    "$$\n",
    "(\\psi(x) - \\psi(y))^T(x -y) \\geq 0.\n",
    "$$\n",
    "\n",
    "Supongamos que $f: \\mathbf{R}^n \\rightarrow \\mathbf{R}$ es una función diferenciable convexa. Muestra que $\\nabla f$ es monótona. ¿Es monotóno cada gradiente de una función convexa?.\n",
    "\n",
    "4 . Sea $D_{kl}$ la divergencia de [Kullback-Leibler](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence). Prueba la siguiente desigualdad:\n",
    "\n",
    "$$\n",
    "D_{kl}(u, v) \\geq 0,\\ \\text{para todo}\\ u, v \\in \\mathbf{R}_{++}^n.\n",
    "$$\n",
    "\n",
    "Prueba que $D_{kl}(u ,v) = 0$ si y sólo si $u = v$.\n",
    "\n",
    "Sugerencia: La divergencia de Kullbacker-Leibler puede ser expresado como,\n",
    "\n",
    "$$\n",
    "D_{kl}(u ,v) = f(u) -f(v) -\\nabla f(v)^T(u -v)\n",
    "$$\n",
    "\n",
    "donde $f(v) = \\sum_{i =1}^{n}v_i\\log v_i$ es la entropia negativa de $v$.\n",
    "\n",
    "5 . Resuelve\n",
    "\n",
    "* Muestra que $f(x) = \\sum_{i =1}^r\\alpha_ix_{[i]}$ es una función convexa de $x$, donde $\\alpha_1 \\geq \\alpha_2 \\geq \\cdots \\geq \\alpha_r$ y $x_{[i]}$ denota la i-ésima componente más grande de $x$.\n",
    "\n",
    "* Sea $T(x,w)$ el polinomio trigonométrico\n",
    "\n",
    "$$\n",
    "T(x, w) = x_1 + x_2\\cos w + x_3\\cos 2w + \\dots + x_n\\cos(n - 1)w.\n",
    "$$\n",
    "\n",
    "Muestra que la función\n",
    "\n",
    "$$\n",
    "f(x) = -\\int_{0}^{2\\pi}\\log T(x, w) dw\n",
    "$$\n",
    "\n",
    "es convexa en $\\{x \\in \\mathbf{R}^n\\ |\\ T(x,w) > 0, \\ 0 \\leq w \\leq 2\\pi \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tus respuestas\n",
    "\n",
    "#### Solución 4.1\n",
    "\n",
    "Como f es una función convexa entonces : \n",
    "\n",
    "$$\n",
    "f(\\lambda y +(1 - \\lambda)x ) \\leq \\lambda f(y) + (1 - \\lambda ) f(x) = f(x) + \\lambda (f(x) - f(y) )\n",
    "\\\\\n",
    "Reemplazando : \n",
    "\\int_{0}^{1} f(x + \\lambda (x - y ) ) dx \\leq \\int_{0}^{1} f(x) + \\lambda ( f(x) - f(y) ) = f(x) + \\frac{f(x) - f(y)}{2} = \\frac{f(x) + f(y)}{2}\n",
    "$$\n",
    "\n",
    "#### Solución 4.3\n",
    "Como f es convexa entonces por la condición de primer orden\n",
    "$$\n",
    "f(y) \\geq f(x) + \\nabla f(y)^{T}(x - y) \n",
    "\\\\\n",
    "f(x) \\geq f(y) + \\nabla f(x)^{T}(y - x )\n",
    "$$\n",
    "\n",
    "Sumando las 2 desigualdades\n",
    "$$\n",
    "\\nabla f(y)^{T}(x - y)  + \\nabla f(x)^{T}(y - x ) \\leq 0\n",
    "\\\\\n",
    "(\\nabla f(x)^{T} - \\nabla f(y)^{T} ) (x - y ) \\geq 0 \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problemas 5\n",
    "\n",
    "1 . Prueba que la entropia negativa definida es 1- fuertemente convexa con respecto a la norma $\\Vert \\cdot \\Vert_1$ sobre el simplex.\n",
    "\n",
    "Sugerencia: Primera muestra que $\\phi(t) = (t -1)\\log t -2\\frac{(t -1)^2}{t + 1} \\geq 0$ para todo $t \\geq 0$. Ahora sustituye $t = x_i/y_i$ para mostrar que\n",
    "\n",
    "$$\n",
    "\\sum_i(x_i - y_i)\\log \\frac{x_i}{y_i} \\geq \\Vert x - y\\Vert_1^2.\n",
    "$$\n",
    "\n",
    "2 . Calcula la conjugada de Fenchel de las siguiente funciones\n",
    "\n",
    "\\begin{align*}\n",
    "f(x) &= \\begin{cases}\n",
    "0 &\\ \\text{si}\\ x \\in C,\\ \\text{si es C es un conjunto convexo} \\\\\n",
    "\\infty &\\ \\text{en otro caso}.\n",
    "\\end{cases} \\\\\n",
    "f(x) &= ax + b \\\\\n",
    "f(x) &= \\frac{1}{2}x^TAx\\ \\text{donde A es una matriz definida positiva}\\\\\n",
    "f(x) &= -\\log (x)\\\\\n",
    "f(x) & =\\exp(x) \\\\\n",
    "f(x) & = x\\log(x)\n",
    "\\end{align*}\n",
    "\n",
    "3 .Si $f$ tiene un gradiente continuo Lispchitz con módulo L, entonces podemos mostrar las siguientes propiedades.\n",
    "\n",
    "\\begin{align*}\n",
    "f(x^{'}) \\leq f(x) + \\langle x^{'} -x, \\nabla f(x)\\rangle + \\frac{L}{2}\\Vert x - x^{'}\\Vert^2 \\\\\n",
    "f(x^{'}) \\geq f(x) + \\langle x^{'} -x, \\nabla f(x)\\rangle + \\frac{1}{2L}\\Vert \\nabla f(x) - \\nabla f(x^{'})\\Vert^2 \\\\\n",
    "\\langle x^{'} -x, \\nabla f(x) -\\nabla f(x^{'}) \\rangle \\leq L\\Vert x - x^{'} \\Vert^2  \\\\\n",
    "\\langle x^{'} -x, \\nabla f(x) -\\nabla f(x^{'}) \\rangle \\geq  \\frac{1}{L} \\Vert \\nabla f(x) - \\nabla f(x^{'}) \\Vert^2  \\\\.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "4 . Sea $f$,  la norma p-cuadrada\n",
    "\n",
    "$$f(x) = \\frac{1}{2}\\Vert x\\Vert^2_p = \\frac{1}{2}\\Biggl(\\sum_{i}x_i^p\\Biggr)^{2/p}$$.\n",
    "\n",
    "Verifica que la i-ésima componente del gradiente $\\nabla f(x)$ es\n",
    "\n",
    "$$\n",
    "\\nabla_{x_i}f(x) = \\frac{\\text{sign}(x_i)\\vert x_i\\vert^{p -1}}{\\Vert x \\Vert_p^{p -2}}.\n",
    "$$\n",
    "\n",
    "y que la correspondiente divergencia de Bregman es\n",
    "\n",
    "$$\n",
    "\\Delta_f(x, x^{'}) = \\frac{1}{2}\\Vert x \\Vert_p^2 - \\frac{1}{2}\\Vert x^{'}\\Vert_p^2 - \\sum_{i}(x_i -x^{'}_i) \\frac{\\text{sign}(x_i^{'})\\vert x_i^{'}\\vert^{p -1}}{\\Vert x^{'}\\Vert_p^{p -2}}.\n",
    "$$\n",
    "\n",
    "5 . Supongamos $f: \\mathbf{R}^n \\rightarrow \\mathbf{R}$ es convexa y dos veces continuamente diferenciable. Supongamos que $\\overline{y}$ y $\\overline{x}$ están relacionadas por $\\overline{y} = \\nabla f(\\overline{x})$ y $\\nabla^2f(\\overline{x}) \\succ 0$. Muestra \n",
    "\n",
    "* $\\nabla f^{*}(\\overline{y}) = \\overline{x}$.\n",
    "\n",
    "* $\\nabla^2 f^{*}(\\overline{y}) = \\nabla^2f(\\overline{x})^{-1}$.\n",
    "\n",
    "Donde $f^{*}$ es la conjugada de Fenchel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tus respuestas\n",
    "\n",
    "#### Solución 5.4\n",
    "\n",
    "Se sabe que: $ \\nabla_{x_i}f(x) = \\frac{\\partial f(x)}{\\partial x_i} $, entonces:\n",
    "\n",
    "$$ \n",
    "\\nabla_{x_i}f(x) = \\frac{1}{2}\\frac{2}{p}\\Biggl(\\sum_{i}x_i^p\\Biggr)^{\\frac{2}{p}-1}p(x_i^{p-1}) , p \\neq 0 \\\\\n",
    "\\nabla_{x_i}f(x) = \\frac{x_i^{p-1}}{\\Biggl(\\sum_{i}x_i^p\\Biggr)^{\\frac{p-2}{p}}} \\\\\n",
    "\\nabla_{x_i}f(x) = \\frac{x_i^{p-1}}{\\Vert x\\Vert^{p-2}_p} \n",
    "$$\n",
    "\n",
    "Es decir, $\\nabla_{x_i}f(x)= \\frac{\\text{sign}(x_i)\\vert x_i\\vert^{p -1}}{\\Vert x \\Vert_p^{p -2}}$\n",
    "\n",
    "Además, se sabe que: $ \\Delta_f(x, x') = f(x)−f(x')−f'(x')(x−x') $\n",
    "\n",
    "Donde $f' = \\nabla f$, entonces se tiene que:\n",
    "\n",
    "$$ \n",
    "\\Delta_f(x, x') = f(x)−f(x')−\\nabla f(x')(x−x') \\\\\n",
    "\\Delta_f(x, x') = \\frac{1}{2}\\Vert x\\Vert^2_p−\\frac{1}{2}\\Vert x'\\Vert^2_p−\\sum_{i}\\frac{\\text{sign}(x'_i)\\vert x'_i\\vert^{p -1}}{\\Vert x'\\Vert_p^{p -2}} (x_i -x'_i) \n",
    "$$\n",
    "\n",
    "**Referencia:** https://mathoverflow.net/questions/17790/are-bregman-divergences-quasi-convex\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lista de ejercicios\n",
    "\n",
    "Las referencias a los ejercicios son :\n",
    "\n",
    "   * Asignaciones de Andrew Ng de Stanford University y cursos en Coursera.\n",
    "   * Numerical Optimization: Jorge Nocedal, Stephen Wright Springer Series in Operations Research and Financial Engineering) 2nd Edition.\n",
    "   * David Barber [Bayesian reasoning and machine learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online).\n",
    "   \n",
    "   * Introduction to Machine Learning Alex Smola and S.V.N. Vishwanathan Cambridge University Press 2008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 . El descenso de coordenadas es conceptualmente el algoritmo más simple para minimizar una función convexa suave multidimensional $J: \\mathbf{R}^n \\rightarrow \\mathbf{R}$. En cada iteración, seleccione una coordenada, digamos $i$, y actualizamos\n",
    "\n",
    "$$\n",
    "w_{t +1} = w_t -n_te_i\n",
    "$$\n",
    "\n",
    "donde $e_i$ denota el i-ésimo vector de la base estándar en $\\mathbf{R}^n$, mientras que $n_t \\in \\mathbf{R}$ es un tamaño de paso escalar no negativo.\n",
    "\n",
    "Si no se requiere una solución de alta precisión, como ocurre en algunas aplicaciones del Machine learning, a menudo se utiliza el descenso de coordenadas porque a) el coste por iteración es muy bajo y b) la velocidad de convergencia puede ser aceptable especialmente si las variables están ligeramente acopladas.\n",
    "\n",
    "El descenso gradiente  (también conocido como el descenso más pronunciado) es una técnica de optimización para minimizar las funciones objetivo convexas suaves multidimensionales de la forma $J: \\mathbf{R}^n \\rightarrow \\mathbf{R}$. La idea básica es la siguiente: Dada una ubicación $w_t$ en la iteración $t$, calculamos el gradiente $\\nabla J(w_t)$ y actualizamos\n",
    "\n",
    "$$\n",
    "w_{t +1} = w_t -n_t\\nabla J(w_t),\n",
    "$$\n",
    "\n",
    "donde $n_t$ es un salto de paso. Implementa el siguiente algoritmo del descenso de gradiente:\n",
    "\n",
    "**Entrada:** Punto inicial $w_0$, tolerencia de la norma del gradiente $\\epsilon$\n",
    "\n",
    "Ponemos  $t = 0$\n",
    "\n",
    "**while** $\\Vert \\nabla J(w_t) \\Vert \\geq  \\epsilon$ **do** \n",
    "\n",
    "$\\qquad w_t= w_t -n_t\\nabla J(w_t)$\n",
    "  \n",
    "$\\qquad t = t +1$\n",
    " \n",
    "** end while**\n",
    "\n",
    "**Salida $w_t$**\n",
    "\n",
    "\n",
    "\n",
    "* Supongamos que $J$ tiene un gradiente continuo Lipschitz con módulo $L$. Entonces el algoritmo anterior con un salto de paso fijo $n_t = \\frac{1}{L}$ retorna una solución $w_t$ con $\\Vert \\nabla J(w_t) \\Vert \\leq \\epsilon$ en a lo más $O(1/\\epsilon^2)$ iteraciones.\n",
    "\n",
    "* Con las condiciones del problema anterior y asumiendo que $J$ es $\\sigma$ fuertemente convexa y sea $c = 1 - \\frac{\\sigma}{L}$. Entonces $J(w_t) - J(w^*) \\leq \\epsilon$ después de a lo sumo\n",
    "\n",
    "$$\n",
    "\\frac{\\log ((J(w_0) -J(w^{*}))/\\epsilon)}{\\log(1/c)}\n",
    "$$\n",
    "\n",
    "iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 . Para el gradiente conjugado. Consideremos la función cuadrática objetivo, \n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2}w^TAw -bw\n",
    "$$\n",
    "\n",
    "donde $A \\in \\mathbf{R}^{n \\times n}$ es una matriz definida positiva y $b \\in \\mathbf{R}^n$ es un vector arbitrario. Desde que $\\nabla J(w) = Aw -b$, en el óptimo $\\nabla J(w) = 0$, tenemos el sistema $Aw = b$.\n",
    "\n",
    "La actualización w a lo largo de la dirección del gradiente negativo puede llevar a moverse en zigzag. Por lo tanto CG utiliza las denominadas direcciones conjugadas. Esto es, dos vectores $p_t$ y $p_{t^{'}}$ se dicen que son conjugados con respecto a la matriz definida positiva simétrica $A$ si $p^T_{t^{'}}Ap_t = 0$ si $t \\neq t^{'}$.\n",
    "\n",
    "* Prueba que las direcciones conjugadas $\\{p_0, \\dots, p_{n -1} \\}$ son linealmente independientes y forman una base.\n",
    "\n",
    "Las direcciones conjugadas se pueden generar iterativamente como sigue: Comenzando con cualquier $w_0 \\mathbf{R}^n$ y definamos $p_0 = -g_0 = b - Aw_0$ y escribimos,\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha_t &= -\\frac{g_t^Tp_t}{p_t^TAp_t} \\quad \\ \\qquad (1) \\\\\n",
    "w_{t +1} &= w_t + \\alpha_t p_t \\quad \\ \\qquad (2)\\\\\n",
    "g_{t +1} &= Aw_{t +1} -b \\quad \\qquad \\ (3)\\\\\n",
    "\\beta_{t +1} &= -\\frac{g_{t +1}^TAp_t}{p_t^TAp_t} \\ \\quad \\qquad (4) \\\\\n",
    "p_{t +1} &= -g_{t +1} -\\beta_{t +1}p_t \\qquad (5)\n",
    "\\end{align}\n",
    "\n",
    "* Prueba que las anteriores iteraciones del método del gradiente conjugado, convergen al minimizados de $J(w) = \\frac{1}{2}w^TAw -bw$, después de a lo más $n$ pasos.\n",
    "\n",
    "* Implementa el algoritmo del gradiente conjugado\n",
    "\n",
    "\n",
    "**Entrada:** Punto inicial $w_0$, tolerencia de la norma del gradiente $\\epsilon$\n",
    "\n",
    "Ponemos  $t = 0, g_0 = Aw_0 -b$ y $p_0 = -g_0$\n",
    "\n",
    "**while** $\\Vert Aw_t -b \\Vert \\geq  \\epsilon$ **do** \n",
    "\n",
    "\n",
    "\n",
    "$\\qquad \\alpha_t = \\frac{g_t^Tp_t}{p_t^TAp_t}$\n",
    "\n",
    "\n",
    "$\\qquad w_{t +1} = w_t + \\alpha_t p_t$\n",
    "\n",
    "$\\qquad g_{t +1} = g_{t} + \\alpha_t Ap_t$\n",
    "\n",
    "$\\qquad \\beta_{t +1} = \\frac{g_{t +1}^Tg_{t + 1}}{g_t^Tg_t}$\n",
    "\n",
    "$\\qquad p_{t +1} = -g_{t +1} + \\beta_{t +1}p_t$\n",
    "\n",
    "$\\qquad t = t + 1$\n",
    "\n",
    "** end while**\n",
    "\n",
    "**Salida $w_t$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 . Si tenemos $S = \\{(x_i, y_1), \\dots (x_m, y_m) \\}$ el conjunto de entrenamientos de ejemplos, donde cada $x_i \\in \\mathbf{R}^n$, $y_i = \\{\\pm 1 \\}$. Decimos que es conjunto de entrenamiento es linealmente separable, si existe un semiespacio, $(w,b)$ tal que $y_i = \\text{sign}(\\langle w, x_i \\rangle + b)$ para todo $i$. Alternativamente esta condición puede ser escrita como\n",
    "\n",
    "$$\n",
    "\\forall i \\in [m],\\  y_i(\\langle w, x_i \\rangle + b) > 0\n",
    "$$\n",
    "\n",
    "* Muestra que la distancia de un punto $x$  al hiperplano definido por $(w,b)$ donde $\\Vert w \\Vert = 1$ es $\\vert \\langle w, x \\rangle + b \\vert$.\n",
    "\n",
    "Sobre la base de la anterior precedente, el punto más cercano en el conjunto de entrenamiento al hiperplano de separación es $\\min_{i \\in [m]} \\vert \\langle w, x_i \\rangle + b\\vert$. Por tanto la regla fuerte SVM es\n",
    "\n",
    "$$\n",
    "\\text{argmax}_{(w,b): \\Vert w \\Vert = 1}\\min_{i \\in [m]} \\vert \\langle w, x_i \\rangle + b\\vert \\quad \\text{siempre que}\\quad   \\forall i ,\\  y_i(\\langle w, x_i \\rangle + b) > 0\n",
    "$$\n",
    "\n",
    "* Siempre que haya una solución al problema anterior (es decir, estamos en el caso separable), podemos escribir un problema equivalente como sigue\n",
    "\n",
    "$$\n",
    "\\text{argmax}_{(w,b): \\Vert w \\Vert = 1}\\min_{i \\in [m]} y_i(\\langle w, x_i \\rangle + b).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 . Sea la regla fuerte SVM como un problema de optimización cuadrática\n",
    "\n",
    "**Entrada:** $(x_1, y_1), \\dots, (x_m, y_m)$\n",
    "\n",
    "\n",
    "**Resuelve: **  \n",
    "\n",
    "$\\qquad (w_0, b_0) = \\text{argmin}_{(w,b)}\\Vert w \\Vert^2 \\  \\text{siempre que}\\  \\forall i ,\\  y_i(\\langle w, x_i \\rangle + b) \\geq  1$\n",
    " \n",
    "\n",
    "**Salida** $\\hat{w} = \\frac{w_0}{\\Vert w_0 \\Vert}, \\ \\hat{b} = \\frac{b_0}{\\Vert w_0 \\Vert}$.\n",
    "\n",
    "\n",
    "Prueba que la salida de la regla fuerte SMV es una solución de la ecuación\n",
    "\n",
    "$$\n",
    "\\text{argmax}_{(w,b): \\Vert w \\Vert = 1}\\min_{i \\in [m]} y_i(\\langle w, x_i \\rangle + b).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\\begin{equation} g({\\mathbf{z}}) =\\begin{cases} 1 & \\text{if $\\mathbf{z} \\ge 0$}\\\\ -1 & \\text{en otros casos}. \\end{cases} \\end{equation}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
