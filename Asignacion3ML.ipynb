{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignacion 3\n",
    "\n",
    "Todos los asignaciones serán presentadas utilizando los cuadernos de [Jupyter Notebook](http://jupyter.org/), además de respectivas pruebas como ejemplo, así como el uso de mediciones de velocidad de ejecución utilizando el comando `timeit` y algunos gráficos si fuese necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preguntas 1\n",
    "\n",
    "\n",
    "1 . Sea $C \\subseteq \\mathbf{R}^n$ un conjunto convexo, con $x_1, x_2, \\dots x_k \\in C$ y sean $\\theta_1, \\dots \\theta_k \\in \\mathbb{R}, \\theta_1 + \\dots + \\theta_k = 1$. Muestra que $\\theta_1x_1 + \\dots  \\theta_kx_k \\in C$.\n",
    "\n",
    "2 .Demuestra que un conjunto es convexo si y sólo si su intersección con cualquier línea es convexa. Demuestra que un conjunto es afín si y sólo si su intersección con cualquier línea es afín.\n",
    "\n",
    "3 . Convexidad del punto medio. Un conjunto C es convexo del punto medio, si para cada dos puntos $a$, $b$ están en $C$, el punto medio o medio $(a + b)/2$ está en C. \n",
    "\n",
    "Obviamente un conjunto convexo es  convexo medio. Se puede probar que bajo condiciones suaves la convexidad del punto medio implica convexidad. Como un caso simple, prueba  que si C es cerrado y convexo medio, entonces C es convexo.\n",
    "\n",
    "4 .Demuestra que la cápsula convexo de un conjunto S es la intersección de todos los conjuntos convexos que contienen S.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preguntas 2\n",
    "\n",
    "1 .¿ Cuál es la distancia entre dos hiperplanos paralelos $\\{ x \\in \\mathbf{R}^n\\ |\\   a^Tx =b_1 \\}$ y $\\{ x \\in \\mathbf{R}^n \\ |\\  a^Tx =b_2 \\}$?.\n",
    "\n",
    "2 . Sean $a$ y $b$ dos puntos distintos en $\\mathbf{R}^n$, muestra que el conjunto de puntos que satisface. Demuestra que el conjunto de todos los puntos que están más cerca (en la norma euclidiana) $a$ que a $b$, esto es, $\\{x\\ | \\Vert x -a \\Vert_2 \\leq \\Vert x -b \\Vert_2$, es un semiespacio. \n",
    "\n",
    "Describe explicítamente como una desigualdad de la forma $c^Tx \\leq d$. Realiza un gráfico.\n",
    "\n",
    "3 . Sea $x_0, \\dots, x_K \\in \\mathbf{R}^n$. Considera el conjunto de todos los puntos que están más cerca (en la norma euclidiana) $x_0$ que a $x_i$, esto es\n",
    "\n",
    "$$\n",
    "    V = \\{x \\in \\mathbf{R}^n\\ |\\ \\Vert x -x_0 \\Vert_2 \\leq \\Vert x -x_i\\Vert_2,\\ i =1, \\dots, K.\n",
    "$$\n",
    "\n",
    "V es llamada región de Voronoi alrededor de $x_0$ con respecto a $x_1,\\dots, x_K$.\n",
    "\n",
    "Muestra que V es un poliedro. Expresa V en la forma $V = \\{x\\ |\\ Ax \\preceq b \\}$.\n",
    "\n",
    "Por el contrario, dado un poliedro P con interior no vacío, muestra cómo encontrar $x_0, \\dots x_K$ para que el poliedro sea la región de Voronoi de $x_0$ con respecto a $x_1,\\dots, x_K$.\n",
    "\n",
    "4 . $C \\subseteq \\mathbf{R}^n$ el conjunto solución de una desigualdad cuadrática,\n",
    "\n",
    "$$\n",
    "C = \\{ x \\in \\mathbf{R}^n \\ |\\ x^TAx + b^Tx + c \\leq 0 \\},\n",
    "$$\n",
    "\n",
    "donde $A \\in \\mathbf{S}^n, b \\in \\mathbf{R}^n$ y $c \\in \\mathbf{R}$.\n",
    "\n",
    "\n",
    "* Muestra que C es convexo si $A \\succeq 0$.\n",
    "\n",
    "* Muestra que la intersección de C y el hiperplano definido por $g^Tx + h = 0$ (donde $g \\neq 0)$ es convexa si $A + \\lambda gg^T$ si $A +gg^T \\succeq 0$ para algún $\\lambda \\in \\mathbf{R}$.\n",
    "    \n",
    "5 . Sea X una variable aleatoria de valor real con $\\mathbf{P}(x = a_i) = p_i, i = 1,\\dots,n$, donde $a_1 < a_2 < \\cdots < a_n$. \n",
    "\n",
    "De esta manera $p\\in \\mathbf{R}^n$ se encuentra en la probabilidad estándar simplex $PS = \\{p\\ |\\ \\mathbf{1}^Tp = 1, p\\succeq  0\\}$. \n",
    "\n",
    "¿Cuáles de las siguientes condiciones son convexas en p?.\n",
    "\n",
    "* $\\alpha  \\leq \\mathbf{E}f(x) \\leq \\beta$, donde $\\mathbf{E}f(x)$ es el valor esperado, esto es, $\\mathbf{E}f(x) = \\sum_{i =1}^np_if(a_i)$. (La función $f: \\mathbf{R} \\rightarrow \\mathbf{R}$ es dada).\n",
    "\n",
    "\n",
    "* $\\mathbf{P}(X > \\alpha) \\leq \\beta$.\n",
    "\n",
    "\n",
    "* $\\mathbf{E}\\vert X^3 \\vert \\leq \\alpha \\mathbf{E}\\vert X \\vert$.\n",
    "\n",
    "\n",
    "* $\\mathbf{E}X^2 \\leq \\alpha $.\n",
    "\n",
    "\n",
    "* $\\mathbf{E}X^2 \\geq \\alpha $.\n",
    "\n",
    "\n",
    "* $\\mathbf{Var}(X) \\leq \\alpha$, donde $\\mathbf{Var}(X) = \\mathbf{E}(X - \\mathbf{E}X)^2$ es la varianza de $X$.\n",
    "\n",
    "\n",
    "* $\\mathbf{Var}(X) \\geq \\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problemas 3\n",
    "\n",
    "1 . Supongamos que $C$ y $D$ son conjuntos disconexos de $\\mathbf{R}^n$. Considera el conjunto $(a,b) \\in \\mathbf{R}^{n+1}$, para el cual $a^T x \\leq b$ para todo $x \\in C$ y $a^Tx \\geq b$ para todo $x \\in D$. Muestra que este es un cono convexo.\n",
    "\n",
    "2 .Supongamos que C y D son dos conjuntos convexos que no se intersecan, es decir, $C \\cap D = \\emptyset$.\n",
    "\n",
    "Entonces existe un $a \\neq 0$ y $b$ tal que  $a^Tx \\leq  b$ para todo $x\\in C$ y un $A^Tx \\geq b$ para todo $x \\in D$. En otras palabras, la función afín $a^Tx - b$ no es positiva en C y no negativa en D.\n",
    "\n",
    "El hiperplano $\\{x\\ |\\ a^T x = b\\}$ se llama un hiperplano de separación para los conjuntos C y D o se dice que separa los conjuntos C y D\n",
    "\n",
    "3 .Escribe  un ejemplo de dos conjuntos convexos cerrados que son disjuntos pero no pueden ser estrictamente separados.\n",
    "\n",
    "4 .La función soporte de un conjunto $C \\subseteq \\mathbf{R}^n$ es definida como\n",
    "\n",
    "$$\n",
    "S_C(y) = \\sup\\{y^Tx\\ |\\ x \\in C\\}\n",
    "$$\n",
    "\n",
    "\n",
    "Supongamos que C y D son conjuntos convexos cerrados en $\\mathbf{R}^n$. Demuestra que $C = D$ si y sólo si sus funciones de soporte son iguales.\n",
    "\n",
    "5 .Supongamos que el conjunto C es cerrado, que tiene  interior no vacío y tiene un hiperplano de soporte en cada punto de su frontera. Demuestre que C es convexo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problemas 4\n",
    "\n",
    "1 .Muestra que una función continua $f:\\mathbf{R}^n \\rightarrow \\mathbf{R}^n$ es convexa si y sólo si para cada segmento lineal, el valor promedio sobre el segmento es menor o igual al promedio de los valores en los extremos de los segmentos. Para cada $x, y \\in \\mathbf{R}^n$,\n",
    "\n",
    "$$\n",
    "\\int_{0}^{1}f(x + \\lambda(y -x)d \\lambda \\leq \\frac{f(x) + f(y)}{2}.\n",
    "$$\n",
    "\n",
    "2 .Prueba que una función $f$ dos veces diferenciable es convexa si y sólo si su dominio es convexo y $\\nabla^2f(x) \\succeq 0$ para todo $x$ en el dominio de $f$.\n",
    "\n",
    "3 .Una función $\\psi: \\mathbf{R}^n \\rightarrow \\mathbf{R}^n$ es llamada monótona si para todo $x, y $ en el dominio $\\psi$,\n",
    "\n",
    "$$\n",
    "(\\psi(x) - \\psi(y))^T(x -y) \\geq 0.\n",
    "$$\n",
    "\n",
    "Supongamos que $f: \\mathbf{R}^n \\rightarrow \\mathbf{R}$ es una función diferenciable convexa. Muestra que $\\nabla f$ es monótona. ¿Es monotóno cada gradiente de una función convexa?.\n",
    "\n",
    "4 . Sea $D_{kl}$ la divergencia de [Kullback-Leibler](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence). Prueba la siguiente desigualdad:\n",
    "\n",
    "$$\n",
    "D_{kl}(u, v) \\geq 0,\\ \\text{para todo}\\ u, v \\in \\mathbf{R}_{++}^n.\n",
    "$$\n",
    "\n",
    "Prueba que $D_{kl}(u ,v) = 0$ si y sólo si $u = v$.\n",
    "\n",
    "Sugerencia: La divergencia de Kullbacker-Leibler puede ser expresado como,\n",
    "\n",
    "$$\n",
    "D_{kl}(u ,v) = f(u) -f(v) -\\nabla f(v)^T(u -v)\n",
    "$$\n",
    "\n",
    "donde $f(v) = \\sum_{i =1}^{n}v_i\\log v_i$ es la entropia negativa de $v$.\n",
    "\n",
    "5 . Resuelve\n",
    "\n",
    "* Muestra que $f(x) = \\sum_{i =1}^r\\alpha_ix_{[i]}$ es una función convexa de $x$, donde $\\alpha_1 \\geq \\alpha_2 \\geq \\cdots \\geq \\alpha_r$ y $x_{[i]}$ denota la i-ésima componente más grande de $x$.\n",
    "\n",
    "* Sea $T(x,w)$ el polinomio trigonométrico\n",
    "\n",
    "$$\n",
    "T(x, w) = x_1 + x_2\\cos w + x_3\\cos 2w + \\dots + x_n\\cos(n - 1)w.\n",
    "$$\n",
    "\n",
    "Muestra que la función\n",
    "\n",
    "$$\n",
    "f(x) = -\\int_{0}^{2\\pi}\\log T(x, w) dw\n",
    "$$\n",
    "\n",
    "es convexa en $\\{x \\in \\mathbf{R}^n\\ |\\ T(x,w) > 0, \\ 0 \\leq w \\leq 2\\pi \\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problemas 5\n",
    "\n",
    "1 . Prueba que la entropia negativa definida es 1- fuertemente convexa con respecto a la norma $\\Vert \\cdot \\Vert_1$ sobre el simplex.\n",
    "\n",
    "Sugerencia: Primera muestra que $\\phi(t) = (t -1)\\log t -2\\frac{(t -1)^2}{t + 1} \\geq 0$ para todo $t \\geq 0$. Ahora sustituye $t = x_i/y_i$ para mostrar que\n",
    "\n",
    "$$\n",
    "\\sum_i(x_i - y_i)\\log \\frac{x_i}{y_i} \\geq \\Vert x - y\\Vert_1^2.\n",
    "$$\n",
    "\n",
    "2 . Calcula la conjugada de Fenchel de las siguiente funciones\n",
    "\n",
    "\\begin{align*}\n",
    "f(x) &= \\begin{cases}\n",
    "0 &\\ \\text{si}\\ x \\in C,\\ \\text{si es C es un conjunto convexo} \\\\\n",
    "\\infty &\\ \\text{en otro caso}.\n",
    "\\end{cases} \\\\\n",
    "f(x) &= ax + b \\\\\n",
    "f(x) &= \\frac{1}{2}x^TAx\\ \\text{donde A es una matriz definida positiva}\\\\\n",
    "f(x) &= -\\log (x)\\\\\n",
    "f(x) & =\\exp(x) \\\\\n",
    "f(x) & = x\\log(x)\n",
    "\\end{align*}\n",
    "\n",
    "3 .Si $f$ tiene un gradiente continuo Lispchitz con módulo L, entonces podemos mostrar las siguientes propiedades.\n",
    "\n",
    "\\begin{align*}\n",
    "f(x^{'}) \\leq f(x) + \\langle x^{'} -x, \\nabla f(x)\\rangle + \\frac{L}{2}\\Vert x - x^{'}\\Vert^2 \\\\\n",
    "f(x^{'}) \\geq f(x) + \\langle x^{'} -x, \\nabla f(x)\\rangle + \\frac{1}{2L}\\Vert \\nabla f(x) - \\nabla f(x^{'})\\Vert^2 \\\\\n",
    "\\langle x^{'} -x, \\nabla f(x) -\\nabla f(x^{'}) \\rangle \\leq L\\Vert x - x^{'} \\Vert^2  \\\\\n",
    "\\langle x^{'} -x, \\nabla f(x) -\\nabla f(x^{'}) \\rangle \\geq  \\frac{1}{L} \\Vert \\nabla f(x) - \\nabla f(x^{'}) \\Vert^2  \\\\.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "4 . Sea $f$,  la norma p-cuadrada\n",
    "\n",
    "$$f(x) = \\frac{1}{2}\\Vert x\\Vert^2_p = \\frac{1}{2}\\Biggl(\\sum_{i}x_i^p\\Biggr)^{2/p}$$.\n",
    "\n",
    "Verifica que la i-ésima componente del gradiente $\\nabla f(x)$ es\n",
    "\n",
    "$$\n",
    "\\nabla_{x_i}f(x) = \\frac{\\text{sign}(x_i)\\vert x_i\\vert^{p -1}}{\\Vert x \\Vert_p^{p -2}}.\n",
    "$$\n",
    "\n",
    "y que la correspondiente divergencia de Bregman es\n",
    "\n",
    "$$\n",
    "\\Delta_f(x, x^{'}) = \\frac{1}{2}\\Vert x \\Vert_p^2 - \\frac{1}{2}\\Vert x^{'}\\Vert_p^2 - \\sum_{i}(x_i -x^{'}_i) \\frac{\\text{sign}(x_i^{'})\\vert x_i^{'}\\vert^{p -1}}{\\Vert x^{'}\\Vert_p^{p -2}}.\n",
    "$$\n",
    "\n",
    "5 . Supongamos $f: \\mathbf{R}^n \\rightarrow \\mathbf{R}$ es convexa y dos veces continuamente diferenciable. Supongamos que $\\overline{y}$ y $\\overline{x}$ están relacionadas por $\\overline{y} = \\nabla f(\\overline{x})$ y $\\nabla^2f(\\overline{x}) \\succ 0$. Muestra \n",
    "\n",
    "* $\\nabla f^{*}(\\overline{y}) = \\overline{x}$.\n",
    "\n",
    "* $\\nabla^2 f^{*}(\\overline{y}) = \\nabla^2f(\\overline{x})^{-1}$.\n",
    "\n",
    "Donde $f^{*}$ es la conjugada de Fenchel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lista de ejercicios\n",
    "\n",
    "Las referencias a los ejercicios son :\n",
    "\n",
    "   * Asignaciones de Andrew Ng de Stanford University y cursos en Coursera.\n",
    "   * Numerical Optimization: Jorge Nocedal, Stephen Wright Springer Series in Operations Research and Financial Engineering) 2nd Edition.\n",
    "   * David Barber [Bayesian reasoning and machine learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online).\n",
    "   \n",
    "   * Introduction to Machine Learning Alex Smola and S.V.N. Vishwanathan Cambridge University Press 2008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 . El descenso de coordenadas es conceptualmente el algoritmo más simple para minimizar una función convexa suave multidimensional $J: \\mathbf{R}^n \\rightarrow \\mathbf{R}$. En cada iteración, seleccione una coordenada, digamos $i$, y actualizamos\n",
    "\n",
    "$$\n",
    "w_{t +1} = w_t -n_te_i\n",
    "$$\n",
    "\n",
    "donde $e_i$ denota el i-ésimo vector de la base estándar en $\\mathbf{R}^n$, mientras que $n_t \\in \\mathbf{R}$ es un tamaño de paso escalar no negativo.\n",
    "\n",
    "Si no se requiere una solución de alta precisión, como ocurre en algunas aplicaciones del Machine learning, a menudo se utiliza el descenso de coordenadas porque a) el coste por iteración es muy bajo y b) la velocidad de convergencia puede ser aceptable especialmente si las variables están ligeramente acopladas.\n",
    "\n",
    "El descenso gradiente  (también conocido como el descenso más pronunciado) es una técnica de optimización para minimizar las funciones objetivo convexas suaves multidimensionales de la forma $J: \\mathbf{R}^n \\rightarrow \\mathbf{R}$. La idea básica es la siguiente: Dada una ubicación $w_t$ en la iteración $t$, calculamos el gradiente $\\nabla J(w_t)$ y actualizamos\n",
    "\n",
    "$$\n",
    "w_{t +1} = w_t -n_t\\nabla J(w_t),\n",
    "$$\n",
    "\n",
    "donde $n_t$ es un salto de paso. Implementa el siguiente algoritmo del descenso de gradiente:\n",
    "\n",
    "**Entrada:** Punto inicial $w_0$, tolerencia de la norma del gradiente $\\epsilon$\n",
    "\n",
    "Ponemos  $t = 0$\n",
    "\n",
    "**while** $\\Vert \\nabla J(w_t) \\Vert \\geq  \\epsilon$ **do** \n",
    "\n",
    "$\\qquad w_t= w_t -n_t\\nabla J(w_t)$\n",
    "  \n",
    "$\\qquad t = t +1$\n",
    " \n",
    "** end while**\n",
    "\n",
    "**Salida $w_t$**\n",
    "\n",
    "\n",
    "\n",
    "* Supongamos que $J$ tiene un gradiente continuo Lipschitz con módulo $L$. Entonces el algoritmo anterior con un salto de paso fijo $n_t = \\frac{1}{L}$ retorna una solución $w_t$ con $\\Vert \\nabla J(w_t) \\Vert \\leq \\epsilon$ en a lo más $O(1/\\epsilon^2)$ iteraciones.\n",
    "\n",
    "* Con las condiciones del problema anterior y asumiendo que $J$ es $\\sigma$ fuertemente convexa y sea $c = 1 - \\frac{\\sigma}{L}$. Entonces $J(w_t) - J(w^*) \\leq \\epsilon$ después de a lo sumo\n",
    "\n",
    "$$\n",
    "\\frac{\\log ((J(w_0) -J(w^{*}))/\\epsilon)}{\\log(1/c)}\n",
    "$$\n",
    "\n",
    "iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 . Para el gradiente conjugado. Consideremos la función cuadrática objetivo, \n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2}w^TAw -bw\n",
    "$$\n",
    "\n",
    "donde $A \\in \\mathbf{R}^{n \\times n}$ es una matriz definida positiva y $b \\in \\mathbf{R}^n$ es un vector arbitrario. Desde que $\\nabla J(w) = Aw -b$, en el óptimo $\\nabla J(w) = 0$, tenemos el sistema $Aw = b$.\n",
    "\n",
    "La actualización w a lo largo de la dirección del gradiente negativo puede llevar a moverse en zigzag. Por lo tanto CG utiliza las denominadas direcciones conjugadas. Esto es, dos vectores $p_t$ y $p_{t^{'}}$ se dicen que son conjugados con respecto a la matriz definida positiva simétrica $A$ si $p^T_{t^{'}}Ap_t = 0$ si $t \\neq t^{'}$.\n",
    "\n",
    "* Prueba que las direcciones conjugadas $\\{p_0, \\dots, p_{n -1} \\}$ son linealmente independientes y forman una base.\n",
    "\n",
    "Las direcciones conjugadas se pueden generar iterativamente como sigue: Comenzando con cualquier $w_0 \\mathbf{R}^n$ y definamos $p_0 = -g_0 = b - Aw_0$ y escribimos,\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha_t &= -\\frac{g_t^Tp_t}{p_t^TAp_t} \\quad \\ \\qquad (1) \\\\\n",
    "w_{t +1} &= w_t + \\alpha_t p_t \\quad \\ \\qquad (2)\\\\\n",
    "g_{t +1} &= Aw_{t +1} -b \\quad \\qquad \\ (3)\\\\\n",
    "\\beta_{t +1} &= -\\frac{g_{t +1}^TAp_t}{p_t^TAp_t} \\ \\quad \\qquad (4) \\\\\n",
    "p_{t +1} &= -g_{t +1} -\\beta_{t +1}p_t \\qquad (5)\n",
    "\\end{align}\n",
    "\n",
    "* Prueba que las anteriores iteraciones del método del gradiente conjugado, convergen al minimizados de $J(w) = \\frac{1}{2}w^TAw -bw$, después de a lo más $n$ pasos.\n",
    "\n",
    "* Implementa el algoritmo del gradiente conjugado\n",
    "\n",
    "\n",
    "**Entrada:** Punto inicial $w_0$, tolerencia de la norma del gradiente $\\epsilon$\n",
    "\n",
    "Ponemos  $t = 0, g_0 = Aw_0 -b$ y $p_0 = -g_0$\n",
    "\n",
    "**while** $\\Vert Aw_t -b \\Vert \\geq  \\epsilon$ **do** \n",
    "\n",
    "\n",
    "\n",
    "$\\qquad \\alpha_t = \\frac{g_t^Tp_t}{p_t^TAp_t}$\n",
    "\n",
    "\n",
    "$\\qquad w_{t +1} = w_t + \\alpha_t p_t$\n",
    "\n",
    "$\\qquad g_{t +1} = g_{t} + \\alpha_t Ap_t$\n",
    "\n",
    "$\\qquad \\beta_{t +1} = \\frac{g_{t +1}^Tg_{t + 1}}{g_t^Tg_t}$\n",
    "\n",
    "$\\qquad p_{t +1} = -g_{t +1} + \\beta_{t +1}p_t$\n",
    "\n",
    "$\\qquad t = t + 1$\n",
    "\n",
    "** end while**\n",
    "\n",
    "**Salida $w_t$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 . Si tenemos $S = \\{(x_i, y_1), \\dots (x_m, y_m) \\}$ el conjunto de entrenamientos de ejemplos, donde cada $x_i \\in \\mathbf{R}^n$, $y_i = \\{\\pm 1 \\}$. Decimos que es conjunto de entrenamiento es linealmente separable, si existe un semiespacio, $(w,b)$ tal que $y_i = \\text{sign}(\\langle w, x_i \\rangle + b)$ para todo $i$. Alternativamente esta condición puede ser escrita como\n",
    "\n",
    "$$\n",
    "\\forall i \\in [m],\\  y_i(\\langle w, x_i \\rangle + b) > 0\n",
    "$$\n",
    "\n",
    "* Muestra que la distancia de un punto $x$  al hiperplano definido por $(w,b)$ donde $\\Vert w \\Vert = 1$ es $\\vert \\langle w, x \\rangle + b \\vert$.\n",
    "\n",
    "Sobre la base de la anterior precedente, el punto más cercano en el conjunto de entrenamiento al hiperplano de separación es $\\min_{i \\in [m]} \\vert \\langle w, x_i \\rangle + b\\vert$. Por tanto la regla fuerte SVM es\n",
    "\n",
    "$$\n",
    "\\text{argmax}_{(w,b): \\Vert w \\Vert = 1}\\min_{i \\in [m]} \\vert \\langle w, x_i \\rangle + b\\vert \\quad \\text{siempre que}\\quad   \\forall i ,\\  y_i(\\langle w, x_i \\rangle + b) > 0\n",
    "$$\n",
    "\n",
    "* Siempre que haya una solución al problema anterior (es decir, estamos en el caso separable), podemos escribir un problema equivalente como sigue\n",
    "\n",
    "$$\n",
    "\\text{argmax}_{(w,b): \\Vert w \\Vert = 1}\\min_{i \\in [m]} y_i(\\langle w, x_i \\rangle + b).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 . Sea la regla fuerte SVM como un problema de optimización cuadrática\n",
    "\n",
    "**Entrada:** $(x_1, y_1), \\dots, (x_m, y_m)$\n",
    "\n",
    "\n",
    "**Resuelve: **  \n",
    "\n",
    "$\\qquad (w_0, b_0) = \\text{argmin}_{(w,b)}\\Vert w \\Vert^2 \\  \\text{siempre que}\\  \\forall i ,\\  y_i(\\langle w, x_i \\rangle + b) \\geq  1$\n",
    " \n",
    "\n",
    "**Salida** $\\hat{w} = \\frac{w_0}{\\Vert w_0 \\Vert}, \\ \\hat{b} = \\frac{b_0}{\\Vert w_0 \\Vert}$.\n",
    "\n",
    "\n",
    "Prueba que la salida de la regla fuerte SMV es una solución de la ecuación\n",
    "\n",
    "$$\n",
    "\\text{argmax}_{(w,b): \\Vert w \\Vert = 1}\\min_{i \\in [m]} y_i(\\langle w, x_i \\rangle + b).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
